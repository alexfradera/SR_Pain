---
title: "Search terms"
author: "Alex Fradera"
date: "15/06/2021"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Using litsearchr
```{r install packages, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(igraph)
library(litsearchr)
library(ggraph)
library(stringr)
packageVersion("litsearchr")
```
## Functions

```{r launch_scripts, echo=FALSE, fig.show='hide', include=TRUE, warning=FALSE}
# This is the cleaning and pre-processing described above.
# setwd("C:/Users/Alexander Fradera/OneDrive - University of Glasgow/non-dclin/5_Code/SR_Pain/")
 source("SR_functions.R", local = knitr::knit_global(), echo = FALSE)
```


## Completing the naive search

Completed a main search for PubMed pain* AND cognit* and screen* which produced 949 results, and then amended using NOT (cognit* behav* therapy) to limit the irrelevant results (659). I then put this into litsearchr. NB I then found further variant searches and approached these as well.


## Collating search keywords
We can then have a look at the keywords - here limited to those that appear in 3 or more papers

```{r get_keywords}
S1_keywords <-term_extractor(naive_results,1,3)
S1_keywords
```

## Collating search title content

A bit more to do here as titles can contain many "stopwords" that are not informative to the topic of the article. I have begun to build a tentative stopword list at SR_stopwords.txt 

```{r create_stops}

SR_stopwords <- read_lines("SR_stopwords.txt")
all_stopwords <- c(get_stopwords("English"), SR_stopwords)

```

```{r get_best_titles}
S1_titles<- get_best_titles(naive_results)
)
S1_titles
```

We can combine these useful title terms with keywords, removing duplicates:
```{r combined_terms}
S1_terms <- unique(c(S1_keywords, S1_titles))
```


### Network analysis of content
```{r create_network}
docs <- paste(naive_results[, "title"], naive_results[, "abstract"])
dfm <- create_dfm(elements=docs, features=terms)
g <- create_network(dfm, min_studies=1)

```

We can now visualise this:

```{r vis_network}


network_vis<-ggraph(g, layout="stress") +
     coord_fixed() +
     expand_limits(x=c(-3, 3)) +
     geom_edge_link(aes(alpha=weight)) +
     geom_node_point(shape="circle filled", fill="white") +
     geom_node_text(aes(label=name), hjust="outward", check_overlap=TRUE) +
     guides(edge_alpha=FALSE)
network_vis
```

And find the strongest links in the network (note later entries have a stronger link):

```{r network_strengths}

strengths <- strength(g)

data.frame(term=names(strengths), strength=strengths, row.names=NULL) %>%
  mutate(rank=rank(strength, ties.method="min")) %>%
  arrange(strength) ->
  term_strengths

term_strengths
```


This can then be plotted against changepoints (where the network strength shifs, a bit like an eigenvalue)


```{r network_cutoffs}
cutoff_fig <- ggplot(term_strengths, aes(x=rank, y=strength, label=term)) +
  geom_line() +
  geom_point() +
  geom_text(data=filter(term_strengths, rank>5), hjust="right", nudge_y=20, check_overlap=TRUE)

cutoff_change <- find_cutoff(g, method="changepoint", knot_num=3)

cutoff_fig +
  geom_hline(yintercept=cutoff_change, linetype="dashed")

```

By selecting one of these changepoints the set of title-derived keywords can be reduced down:


```{r}
g_redux <- reduce_graph(g, cutoff_change[1]) # tweak this number to change the cutoff option
selected_terms <- get_keywords(g_redux)

selected_terms
```

### find some way of connecting this back with the keywords
The Tudge walkthrough doesn't discuss this. But it seems that we have  information from the keywords that hasn't been involved in the network analysis, but we don't want to discard. Lacking expertise, the simplest thing would be to combine these with the title-derived selected terms just produced. (Also add in some starred terms from the original approach).
```{r longlist}


longlist <- unique(c(keywords, selected_terms))

extra_terms <- c(
  "pain*",
  "cognit*",
  "screen*"
)

longlist <-c(longlist, extra_terms)
longlist

```




## Grouping with litsearchr

The recommendation is to do this by hand - pick out the terms from the list and organise them under your themes.

```{r make_terms_shortlists}
grouped_terms <-list(
  pain=longlist[c(12,13,14,37,44,45,50,54,55,62,63,99)],
 cognition=longlist[c(15:20,24,51,96,100)],
  screen=longlist[c(7,82,93,101)]
)
grouped_terms

```
```{r temptable}
lapply(grouped_terms, write, "test.txt", append=TRUE, ncolumns=1000)
```




```{r write_shortlist, eval=FALSE, include=TRUE}

# record this search into a text file
write_search(
  grouped_terms,
  languages="English",
  exactphrase=TRUE,
  stemming=FALSE, # DECISION: stem or not stem?
  closure="left",
  writesearch=TRUE
)

cat(read_file("search-inEnglish.txt"))
```




\(\("chronic migraine" OR "chronic musculoskeletal pain" OR "chronic pain" OR fibromyalgia OR knee OR "low back pain" OR migraine OR "neuropathic pain" OR nociception OR pain\) AND \(cognition OR cognitive OR dementia OR "mild cognitive impairment"\) AND \(screening\)\)



# Population search terms
Not sure about this: do we need to specify Adult or just use in exclusion criteria?


```{r preview, include=FALSE}

input_file <- "cog_screens.csv"
x <- spec_csv(input_file) # good practice to peek before reading in
x

```

```{r load_file, include=FALSE}
cog_screens <- read_csv(input_file
                     )
```






# Pain search terms


## DRAW SOMETHING FROM LITSEARCHR
### Add further terms



# Cognitive screen search terms

```{r make_abbreviation_vector, include=FALSE}

abbrevsonly <- filter(cog_screens, cog_screens$Abbreviation != "to complete")
```







## Raw search terms


Based on all the cognitive screens in the .csv file, we would need to search for these test names and referents.

```{r text_list, include=TRUE}


screen_full_names<- str_flatten(cog_screens$Test_Name,"\" OR \"")
screen_abbreviations<- str_flatten(abbrevsonly$Abbreviation,"\" OR \"")
screen_referents <- str_c("\"", screen_full_names, "\" OR \"", screen_abbreviations, "\"")
 writeLines(screen_referents)

```
In other words:

`r screen_referents`


*Note:* in fact some of the screeners names will need to be amended with wildcards.

## Putting search terms into readable contexts

### EBSCO Host

```{r make_ebsco_friendly}
s_screen_title <-  str_c("TI (",screen_referents,")")
s_screen_abstract <-  str_c("AB (",screen_referents,")")
  s_screen_testmeasures  <-  str_c("TM (",screen_referents,")")

```


# Finalise everything

```{r write_shortlist2, eval=FALSE, include=TRUE}

# record this search into a text file
final_search <- write_search(
  TO_DEFINE,
  languages="English",
  exactphrase=TRUE,
  stemming=FALSE, # DECISION: stem or not stem?
  closure="left",
  writesearch=FALSE
)

final_search
```

### Feeding this back in to pubmed

We get a new search, which we save in as new_results:

```{r import_new}
new_results <- import_results(file="new_pubmed.nbib")
```


```{r find_discrepancies}
naive_results %>%
  mutate(in_new_results=title %in% new_results[, "title"]) ->
  naive_results

excluded_articles <- naive_results %>%
  filter(!in_new_results) %>%
  select(title, keywords)

```

```{r write_discrepancies, eval=FALSE, include=TRUE}
 write.csv(excluded_articles, file = "excluded_articles.csv")
```




```{r checking_importants}
important_titles <- c(
  "Screening for pain in patients with cognitive and communication difficulties: evaluation of the SPIN-screen",
  "Pain and the Montreal Cognitive Assessment (MoCA) in Aging",
  "Disruption of cognitive function in Fibromyalgia Syndrome",
  "Cognitive Function Impairment in Patients with Neuropathic Pain Under Standard Conditions of Care",
  "Validity and reliability of the clock drawing test as a screening tool for cognitive impairment in patients with fibromyalgia",
  "Tales of the boogeyman"
)

data.frame(check_recall(important_titles, new_results[, "title"]))
```



